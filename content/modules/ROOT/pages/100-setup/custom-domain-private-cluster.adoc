= Deploying Private ARO clusters with Custom Domains
:authors: ["Roberto Carratal√°"]
:date: 2023-09-04
:tags: ["ARO", "DNS"]
:experimental: true

== Overview

By default Azure Red Hat OpenShift uses self-signed certificates for all of the routes created on `*.apps.<random>.<location>.aroapp.io`.

Many companies also seek to leverage the capabilities of Azure Red Hat OpenShift (ARO) to deploy their applications while using their own custom domain.
By utilizing ARO's custom domain feature, companies can ensure hosting their applications under their own domain name.

If we choose to specify a custom domain, for example aro.myorg.com, the OpenShift console will be available at a URL such as `+https://console-openshift-console.apps.aro.myorg.com+`, instead of the built-in domain `+https://console-openshift-console.apps.<random>.<location>.aroapp.io+`.

Furthermore, if we choose Custom DNS, after connecting to the cluster, we will need to configure a custom certificate for our ARO ingress controller and custom certificate of our API server.

== Prerequisites

. Set and verify the following environment variables
+
[source,bash,subs="+macros,+attributes",role=execute]
----
cat << EOF >> ~/.bashrc
export DOMAIN=aro.private.$GUID.azure.redhatworkshops.io <1>
export DOMAIN_PARENT=$GUID.azure.redhatworkshops.io <2>
export NETWORK_SUBNET=10.0.0.0/20
export CONTROL_SUBNET=10.0.0.0/24
export MACHINE_SUBNET=10.0.1.0/24
export FIREWALL_SUBNET=10.0.2.0/24
export JUMPHOST_SUBNET=10.0.3.0/24
export NAMESPACE=aro-custom-domain
export AZR_CLUSTER=aro-custom-private-cluster-$GUID
export AZR_DNS_RESOURCE_GROUP="openenv-$GUID"
export EMAIL=username.taken@gmail.com
EOF
source ~/.bashrc
----
<1> Note the _private_ custom domain `aro.private.$GUID.azure.redhatworkshops.io`
<2> The domain parent is two steps up

== ARO Cluster Networking

Before deploying Azure Red Hat OpenShift (ARO), there are certain prerequisites that need to be fulfilled.

=== Create Resource Group for ARO Cluster

. Create an Azure Resource Group:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az group create                \
  --name $AZR_RESOURCE_GROUP   \
  --location $AZR_RESOURCE_LOCATION
----

=== ARO Networking Prerequisites

. Create Virtual Network
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network vnet create                                    \
  --address-prefixes $NETWORK_SUBNET                      \
  --name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"   \
  --resource-group $AZR_RESOURCE_GROUP
----

. Create Control Plane Subnet
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network vnet subnet create                                     \
  --resource-group $AZR_RESOURCE_GROUP                            \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"      \
  --name "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION" \
  --address-prefixes $CONTROL_SUBNET                              \
  --service-endpoints Microsoft.ContainerRegistry
----

. Create Machine Subnet
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network vnet subnet create                                       \
  --resource-group $AZR_RESOURCE_GROUP                              \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"        \
  --name "$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION"   \
  --address-prefixes $MACHINE_SUBNET                                \
  --service-endpoints Microsoft.ContainerRegistry
----

. https://learn.microsoft.com/en-us/azure/private-link/disable-private-endpoint-network-policy?tabs=network-policy-portal[Disable Network Policies^] for Private Link Service on the control plane subnet
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network vnet subnet update                                       \
  --name "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION"   \
  --resource-group $AZR_RESOURCE_GROUP                              \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"        \
  --disable-private-link-service-network-policies true
----

== Egress

Public and Private clusters will have https://learn.microsoft.com/en-us/cli/azure/aro?view=azure-cli-latest#az-aro-create[--outbound-type] defined to LoadBalancer by default.
It means all clusters by default have open egress to the internet through the public load balancer.

If you want to change the default behavior to restrict the Internet Egress, you have to set `--outbound-type` during the creation of the cluster to `UserDefinedRouting` and use a Firewall solution from your choice of Azure native solutions like Azure Firewall or Azure NAT Gateway.

=== Firewall + Internet Egress

This replaces the routes for the cluster to go through the Firewall for egress vs the LoadBalancer.
It does come with extra Azure costs of course.

WARNING: This step will restrict egress.

. Make sure you have the AZ CLI firewall extensions
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az extension add -n azure-firewall
az extension update -n azure-firewall
----

. Create a firewall network
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network vnet subnet create                                 \
  -g $AZR_RESOURCE_GROUP                                      \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"  \
  -n "AzureFirewallSubnet"                                    \
  --address-prefixes $FIREWALL_SUBNET
----

. Create IP, and firewall
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network public-ip create -g $AZR_RESOURCE_GROUP -n fw-ip   \
  --sku "Standard" --location $AZR_RESOURCE_LOCATION

az network firewall create -g $AZR_RESOURCE_GROUP             \
  -n aro-private -l $AZR_RESOURCE_LOCATION
----

. Configure the firewall and configure IP Config
+
WARNING: This may take as much as 15 minutes to process
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network firewall ip-config create -g $AZR_RESOURCE_GROUP    \
  -f aro-private -n fw-config --public-ip-address fw-ip        \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"
----

. Set firewall environment variables
+
[source,bash,subs="+macros,+attributes",role=execute]
----
FWPUBLIC_IP=$(az network public-ip show -g $AZR_RESOURCE_GROUP -n fw-ip --query "ipAddress" -o tsv)
FWPRIVATE_IP=$(az network firewall show -g $AZR_RESOURCE_GROUP -n aro-private --query "ipConfigurations[0].privateIPAddress" -o tsv)

echo "export FWPUBLIC_IP=$FWPUBLIC_IP" >> ~/.bashrc
echo "export FWPRIVATE_IP=$FWPRIVATE_IP" >> ~/.bashrc
----

. Create and configure a route table
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network route-table create -g $AZR_RESOURCE_GROUP --name aro-udr

sleep 10

az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-udr \
  --route-table-name aro-udr --address-prefix 0.0.0.0/0                   \
  --next-hop-type VirtualAppliance --next-hop-ip-address $FWPRIVATE_IP

az network route-table route create -g $AZR_RESOURCE_GROUP --name aro-vnet   \
  --route-table-name aro-udr --address-prefix 10.0.0.0/16 --name local-route \
  --next-hop-type VirtualNetworkGateway
----

. Update the subnets to use the Firewall
+
Once the cluster is deployed successfully you can update the subnets to use the firewall instead of the default outbound loadbalancer rule.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network vnet subnet update -g $AZR_RESOURCE_GROUP            \
--vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION        \
--name "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION" \
--route-table aro-udr

az network vnet subnet update -g $AZR_RESOURCE_GROUP            \
--vnet-name $AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION        \
--name "$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION" \
--route-table aro-udr
----

== Create Private ARO Clusters with Custom Domain

. Create Private ARO Cluster with Custom Domain
+
____
When the --domain flag with an FQDN (e.g. my.domain.com) is used to create your cluster you will need to configure DNS and a certificate authority for your API server and apps ingress.
This will be done once we establish connectivity to the cluster.
____
+
. Start a tmux session to preserve your work if connection is lost.
Rejoin your tmux session after connection loss with `tmux a`.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
tmux
----
+
////
. Deploy the ARO cluster (no SP creation)
+
WARNING: The next step takes over 45 minutes to complete.
It's best to start `tmux` in case you are disconnected.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
# DOMAIN install (plus UDR)
az aro create \
  --resource-group $AZR_RESOURCE_GROUP \
  --name $AZR_CLUSTER \
  --vnet "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION" \
  --master-subnet "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION" \
  --worker-subnet "$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION" \
  --outbound-type UserDefinedRouting \ <1>
  --apiserver-visibility Private \
  --ingress-visibility Private \
  --pull-secret @$AZR_PULL_SECRET \
  --domain $DOMAIN
----
////

. Find an older, supported version of OpenShift to install
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az aro get-versions
export AZR_OCP_VERSION=$(az aro get-versions --location $AZR_RESOURCE_LOCATION -ojson --query '[0]' -o tsv)
echo "export AZR_OCP_VERSION=$AZR_OCP_VERSION" >> ~/.bashrc
echo $AZR_OCP_VERSION
----
+
NOTE: `az aro create` will default to installing the latest version.
*Copy one of the older version semvers.*
We will run an upgrade later.

. Create an Azure Service Principal
+
This is optional, but useful to separate concerns.
You can just as well use the existing service principal.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
AZ_SUB_ID=$(az account show --query id -o tsv)
AZ_SP_PASS=$(az ad sp create-for-rbac -n "${AZR_CLUSTER}-SP" --role contributor \
  --scopes "/subscriptions/${AZ_SUB_ID}/resourceGroups/${AZR_RESOURCE_GROUP}" \
  --query "password" -o tsv)
AZ_SP_ID=$(az ad sp list --display-name "${AZR_CLUSTER}-SP" --query "[0].appId" -o tsv)
----
+
WARNING: The next step takes over 45 minutes to complete.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
date # start time
az aro create                                                            \
--resource-group $AZR_RESOURCE_GROUP                                     \
--name $AZR_CLUSTER                                                      \
--domain $DOMAIN                                                         \
--version $AZR_OCP_VERSION                                               \
--vnet "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"                    \
--master-subnet "$AZR_CLUSTER-aro-control-subnet-$AZR_RESOURCE_LOCATION" \
--worker-subnet "$AZR_CLUSTER-aro-machine-subnet-$AZR_RESOURCE_LOCATION" \
--apiserver-visibility Private                                           \
--ingress-visibility Private                                             \
--pull-secret @$AZR_PULL_SECRET                                          \
--client-id "${AZ_SP_ID}"                                                \
--client-secret "${AZ_SP_PASS}"                                          \
--outbound-type UserDefinedRouting <1>
----
<1> UserDefinedRouting is essential to Private Clusters

=== Deploy Jumphost During Cluster Installation

As the cluster operates within a private network, it is possible to create a jumphost during the cluster creation process.
This jumphost serves as a secure gateway that allows authorized users to connect to the private cluster environment.

. Open a additional terminal with tmux by pressing kbd:[CTRL+b] then kbd:[c]

. Create Jumphost Subnet
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network vnet subnet create                                \
  --resource-group $AZR_RESOURCE_GROUP                       \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION" \
  --name JumpSubnet                                          \
  --address-prefixes $JUMPHOST_SUBNET                        \
  --service-endpoints Microsoft.ContainerRegistry
----

. Create a Jumphost
+
WARNING: This takes several minutes to complete
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az vm create --name jumphost               \
  --resource-group $AZR_RESOURCE_GROUP     \
  --ssh-key-values $HOME/.ssh/id_rsa.pub   \
  --admin-username aro                     \
  --image "RedHat:RHEL:9_1:9.1.2022112113" \
  --subnet JumpSubnet                      \
  --public-ip-address jumphost-ip          \
  --public-ip-sku Standard                 \
  --generate-ssh-keys                      \
  --vnet-name "$AZR_CLUSTER-aro-vnet-$AZR_RESOURCE_LOCATION"
----

. Save the jump host public IP address
+
[source,bash,subs="+macros,+attributes",role=execute]
----
JUMP_IP=$(az vm list-ip-addresses -g $AZR_RESOURCE_GROUP -n jumphost -o tsv \
--query '[].virtualMachine.network.publicIpAddresses[0].ipAddress')
echo $JUMP_IP
export JUMP_IP=$JUMP_IP
echo "export JUMP_IP=$JUMP_IP" >> $HOME/.bashrc
----

=== Setup sshuttle to gain access the cluster

Your ongoing work that access the cluster needs to be routed through the jump host.
On both your bastion host, and on your laptop, we'll use sshuttle to access the cluster.
sshuttle will route all packets sent to the `10.0.0.8/20` subnet through the jump host.

Let's start with the bastion.

. On the bastion, just run the following command, and it should pick up the proper ssh keys and start sshuttle.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
sshuttle --dns -NHr "aro@${JUMP_IP}" $NETWORK_SUBNET --daemon
----

Now let's setup your laptop.

. To setup `sshuttle` on your laptop
.. Mac Laptops
+
[source,bash,subs="+macros,+attributes",role=execute]
----
if ! command -v brew &> /dev/null
then
   echo "Homebrew is not installed. Installing now..."
   /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
fi
brew install sshuttle
----
+
.. Linux Laptops (RHEL)
+
[source,bash,subs="+macros,+attributes",role=execute]
----
RHEL=$(rpm -E %rhel)
sudo yum install -y "https://dl.fedoraproject.org/pub/epel/epel-release-latest-${RHEL}.noarch.rpm"
sudo yum install -y sshuttle
----
+
.. Windows Laptops
+
[source,bash,subs="+macros,+attributes",role=execute]
----
choco install sshuttle
----

Now you need to configure ssh to do private key authetnication to the jump host.
Copy the private key that was generated on the bastion above to your laptop `~/.ssh/jumphost.pem`

. On the bastion, get the IP of the jump host on the bastion
+
[source,bash,subs="+macros,+attributes",role=execute]
----
echo $JUMP_IP
----
+
. On your laptop, put $JUMP_IP in your envronment
+
[source,bash,subs="+macros,+attributes",role=execute]
----
export JUMP_IP=**** paste the jump host IP here ****
----
+
. Create an entry for the jump host in the file `~/.ssh/config`
+
[source,bash,subs="+macros,+attributes",role=execute]
----
echo "Host ${JUMP_IP}
  IdentityFile ~/.ssh/jumphost.pem
" >> ~/.ssh/config
----
+
. Create the `~/.ssh/jumphost.pem` file by copying it from the bastion to the `~/.ssh/jumphost.pem` file on your laptop.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
nano ~/.ssh/jumphost.pem
----
+
. Write the file and close nano with kbd:[CTRL+x], then kbd:[y], then kbd:[ENTER]
+
. You must also protect this private key file on your laptop by changing its permissions to 600
+
[source,bash,subs="+macros,+attributes",role=execute]
----
chmod 600 ~/.ssh/jumphost.pem
----
+
. Now test ssh to the jump host from your laptop
+
[source,bash,subs="+macros,+attributes",role=execute]
----
ssh aro@${JUMP_IP}
----
+
. You should be logged in OK to your jump host.
Log out of the jump host and return to your laptop by typing `exit`.
+
. Now you can use sshuttle to access the cluster from your latop.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
sshuttle --dns -NHr "aro@${JUMP_IP}" 10.0.0.0/20 --daemon
----
+
. Return to bastion here on the terminal on the right. Switch to the first terminal with the ARO installation to see if installation has finished by pressing kbd:[CTRL+b] then kbd:[n]
+
. It has not finished.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
\ Running ..
----
+
. Hit kbd:[CTRL+b] then kbd:[n] again to return to your second terminal.

Your bastion host and laptop are now ready to access the private cluster that is being deployed right now.

== Configure DNS for the Private ARO Cluster (Ingress Router and API)

////
WARNING: You must wait for ARO to complete installation before proceeding.
Have a beverage. ü´ñ
Hydration is important for health. pass:[&#128161;]
////

It is of utmost important to properly configure DNS for the default ingress router, API server endpoint, and associated routes such as the console and *.apps.

These DNS configurations ensure easy access to the cluster's console, application routes, and APIs, facilitating smooth administration and interaction with the OpenShift/Kubernetes environment.

=== Configure DNS for Default Ingress Router

We need to configure the DNS for the Default Ingress Router (*.apps), to be able to access to the ARO Console, among other things.

. Retrieve the Ingress IP for Azure DNS records
+
[source,bash,subs="+macros,+attributes",role=execute]
----
INGRESS_IP="$(az aro show -n $AZR_CLUSTER -g $AZR_RESOURCE_GROUP --query 'ingressProfiles[0].ip' -o tsv)"
export INGRESS_IP=$INGRESS_IP
echo "export INGRESS_IP=$INGRESS_IP" >> ~/.bashrc
echo $INGRESS_IP
----

==== Apps/Console Public Zone Ingress Configuration

. Create your Azure DNS zone for $DOMAIN
+
NOTE: Alternatively you can use an existing zone if it exists.
You need to have configured your domain name registrar to point to this zone.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network dns zone create \
  -g $AZR_RESOURCE_GROUP \
  -n $DOMAIN

az network dns zone create \
  --parent-name $DOMAIN_PARENT \
  -g $AZR_DNS_RESOURCE_GROUP \
  -n $DOMAIN
----

. Add a record type A pointing the "*.apps.DOMAIN" to the Ingress LB IP, that is the Azure LB that balances the ARO/OpenShift Routers (Haproxies)
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network dns record-set a add-record \
  -g $AZR_DNS_RESOURCE_GROUP \
  -z $DOMAIN \
  -n '*.apps' \
  -a $INGRESS_IP
----

. Adjust Default TTL from 1 Hour (choose an appropriate value, here 5 mins is used)
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network dns record-set a update \
  -g $AZR_DNS_RESOURCE_GROUP \
  -z $DOMAIN \
  -n '*.apps' \
  --set ttl=300
----

. Test the *.apps Domain
+
[source,bash,subs="+macros,+attributes",role=execute]
----
dig +short test.apps.$DOMAIN
----

=== Configure DNS for API server endpoint

We need to configure the DNS for the Kubernetes / OpenShift API of the ARO cluster, to be able to access to the ARO API.

. Retrieve the API Server IP for Azure DNS records:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
API_SERVER_IP="$(az aro show -n $AZR_CLUSTER -g $AZR_RESOURCE_GROUP --query 'apiserverProfile.ip' -o tsv)"
echo $API_SERVER_IP
echo "API_SERVER_IP=$API_SERVER_IP" >> ~/.bashrc
----

. Create an `api` A record to point to the Ingress Load Balancer IP:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network dns record-set a add-record \
  -g $AZR_DNS_RESOURCE_GROUP \
  -z $DOMAIN \
  -n 'api' \
  -a $API_SERVER_IP
----

. Optional (good for initial testing): Adjust default TTL from 1 hour (choose an appropriate value, here 5 mins is used)
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network dns record-set a update \
  -g $AZR_DNS_RESOURCE_GROUP \
  -z $DOMAIN \
  -n 'api' \
  --set ttl=300
----

. Test the api domain:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
dig +short api.$DOMAIN
----

== Generate Let's Encrypt Certificates for API Server and default Ingress Router

The following example employs manually created Let's Encrypt certificates.
However, it's important to note that this is not recommended for production environments unless an automated process has been established for the generation and renewal of these certificates (for instance, through the use of the Cert-Manager operator).

Keep in mind that these certificates are subject to expiry after 90 days.

NOTE: this method relies on public DNS for the issuance of certificates since it uses a DNS challenge.
Once the certificates have been issued, if desired, the public records can be removed (this could be the case if you've created a private ARO cluster and plan to use Azure DNS private record sets).

____
This process uses two terminal sessions.
You will switch between them as you work.
One session runs the Let's Encrypt certificate generation interactive process.
The other session is where you'll updated DNS records for Let's Encrypt to validate.
Don't accidentally stop your ARO installation, which should be in a THIRD terminal.
____

=== Generate LE Certs for default Ingress Router (*.apps/console)

. Create TLS Key Pair for the apps/console domain using certbot
+
[source,bash,subs="+macros,+attributes",role=execute]
----
export SCRATCH_DIR=/tmp/scratch
echo "SCRATCH_DIR=$SCRATCH_DIR" >> ~/.bashrc

certbot certonly --manual \
  --preferred-challenges=dns \
  --email $EMAIL \
  --server https://acme-v02.api.letsencrypt.org/directory \
  --agree-tos \
  --config-dir "$SCRATCH_DIR/config" \
  --work-dir "$SCRATCH_DIR/work" \
  --logs-dir "$SCRATCH_DIR/logs" \
  -d "*.apps.$DOMAIN"
----
+
NOTE: Take note of the Domain and TXT value fields as these are required for Let's Encrypt to validate that you own the domain and can therefore issue you the certificates.
+
WARNING: Don't close or interrupt this process, we will finish after the dns challenge with.
+
. Use your mouse to copy the text record VALUE
+
.Example
----
InY85UGzpDLOiS_xpLp-EXAMPLEzfM47BTAJCx2lN6sA
----

. Switch to the other tmux window with kbd:[CTRL+b] then kbd:[n]

. Paste the DNS_Challenge in the following environment variable
+
[source,bash,subs="+macros,+attributes",role=execute]
----
export APPS_TXT_RECORD="xxxx"
----

. Add the necessary records to validate ownership of the apps domain
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network dns record-set txt add-record \
  -g $AZR_DNS_RESOURCE_GROUP \
  -z $DOMAIN \
  -n "_acme-challenge.apps" \
  -v $APPS_TXT_RECORD
----

. Update the TTL for the records from 1h to 5 minutes to testing purposes
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network dns record-set txt update \
  -g $AZR_DNS_RESOURCE_GROUP \
  -z $DOMAIN \
  -n "_acme-challenge.apps" \
  --set ttl=300
----

. Make sure that you get the TXT record from the Azure domain challenge is registered and propagated properly
+
[source,bash,subs="+macros,+attributes",role=execute]
----
dig +short TXT _acme-challenge.apps.$DOMAIN
----

. Return to the first terminal with tmux by pressing kbd:[CTRL+b] then kbd:[n]

. Finish the generation of the apps certificate PKIs for the ARO cluster by pressing kbd:[Enter]
+
.Example
----
Press Enter to Continue

Successfully received certificate.
----

=== Generate LE Certs for the API

. Create TLS Key Pair for the api domain using certbot:
+
NOTE: Don't close or interrupt this process, we will finish after the dns challenge with the certbot.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
export SCRATCH_DIR=/tmp/scratch

certbot certonly --manual \
  --preferred-challenges=dns \
  --email $EMAIL \
  --server https://acme-v02.api.letsencrypt.org/directory \
  --agree-tos \
  --config-dir "$SCRATCH_DIR/config" \
  --work-dir "$SCRATCH_DIR/work" \
  --logs-dir "$SCRATCH_DIR/logs" \
  -d "api.$DOMAIN"
----

. Open the second terminal from earlier by pressing kbd:[CTRL+b] then kbd:[n]
. Paste the DNS_Challenge value:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
export API_TXT_RECORD="xxxx"
----

. You can add the necessary records to validate ownership of the api domain:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network dns record-set txt add-record \
  -g $AZR_DNS_RESOURCE_GROUP \
  -z $DOMAIN \
  -n "_acme-challenge.api" \
  -v $API_TXT_RECORD
----

. Adjust default TTL from 1 hour (choose an appropriate value, here 5 mins is used):
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network dns record-set txt update \
  -g $AZR_DNS_RESOURCE_GROUP \
  -z $DOMAIN \
  -n "_acme-challenge.api" \
  --set ttl=300
----

. Make sure that you get the TXT record from the Azure domain challenge is registered and propagated properly:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
dig +short TXT _acme-challenge.api.$DOMAIN
----

. Return to the first terminal (where the certbot is) with kbd:[CTRL+b] then kbd:[n], and finish the generation of the API certificate PKIs for the ARO cluster by hitting kbd:[Enter]
+
.Example
----
Press Enter to Continue

Successfully received certificate.
----

=== Configure the Ingress Router with custom certificates

CAUTION: *Now you must wait* for the ARO cluster to finish deploying.
You've done a lot of work.
Look back on this document about what you've done and achieve enlightenment.


By default, the OpenShift Container Platform uses the Ingress Operator to generate an internal Certificate Authority (CA) and issue a wildcard certificate, which is valid for applications under the .apps sub-domain.
This certificate is used by both the web console and CLI.

You can https://docs.openshift.com/container-platform/4.11/security/certificates/replacing-default-ingress-certificate.html[replace the default ingress certificate] for all applications under the .apps subdomain.
After you replace the certificate, all applications, including the web console and CLI, will have encryption provided by specified certificate.

. Configure the API server with custom certificates:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
AROPASS=$(az aro list-credentials --name $AZR_CLUSTER --resource-group $AZR_RESOURCE_GROUP -o tsv --query kubeadminPassword)
AROURL=$(az aro show -g $AZR_RESOURCE_GROUP -n $AZR_CLUSTER --query apiserverProfile.url -o tsv)

echo "export AROPASS=$AROPASS" >> ~/.bashrc
echo "export AROURL=$AROURL" >> ~/.bashrc
----

. Login to the ARO cluster with oc CLI:
+
NOTE: We are currently utilizing the "--insecure-skip-tls-verify=true" flag due to the presence of self-signed certificates in both the API and the default ingress controller (*.apps).
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc login -u kubeadmin -p $AROPASS --server=$AROURL --insecure-skip-tls-verify=true
----

. Create a config map that includes only the root CA certificate used to sign the wildcard certificate:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc create configmap custom-ca \
  --from-file=ca-bundle.crt=$SCRATCH_DIR/config/live/apps.$DOMAIN/fullchain.pem \
  -n openshift-config
----

. Update the cluster-wide proxy configuration with the newly created config map:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc patch proxy/cluster \
  --type=merge \
  --patch='{"spec":{"trustedCA":{"name":"custom-ca"}}}'
----

. Create a secret that contains the wildcard certificate chain and key:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc create secret tls apps-custom-domain \
  --cert=$SCRATCH_DIR/config/live/apps.$DOMAIN/fullchain.pem \
  --key=$SCRATCH_DIR/config/live/apps.$DOMAIN/privkey.pem \
  -n openshift-ingress
----

. Update the Ingress Controller configuration with the newly created secret:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc patch ingresscontroller.operator default \
  --type=merge -p \
  '{"spec":{"defaultCertificate":{"name":"apps-custom-domain"}}}' \
  -n openshift-ingress-operator
----

. Check the OpenShift Ingress pods by watching the pods restart in the openshift-ingress namespace
+
[source,bash,subs="+macros,+attributes",role=execute]
----
watch "oc get pod -n openshift-ingress"
----
+
Hit kbd:[CTRL+c] to exit when they've restarted.

. Verify that your certificate is correctly applied:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
echo | openssl s_client -connect console-openshift-console.apps.$DOMAIN:443 | openssl x509 -noout -text | grep Issuer
----

. Check that the Certificate when you access to the Console is the Cert issued by Let's Encrypt using Certbot:
.. Get the OpenShift Console URL:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc whoami --show-console
----
+
image::aro-custom-domain.png[ARO Custom Domain]

=== Configure the API server with custom certificates

. Create a secret that contains the certificate chain and private key in the openshift-config namespace:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc create secret tls api-custom-domain-cert \
  --cert=$SCRATCH_DIR/config/live/api.$DOMAIN/fullchain.pem \
  --key=$SCRATCH_DIR/config/live/api.$DOMAIN/privkey.pem \
  -n openshift-config
----

. Update the https://docs.openshift.com/container-platform/4.11/security/certificates/api-server.html[API server certificate] to reference the created secret.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
echo $DOMAIN

bash <<EOF
oc patch apiserver cluster \
  --type=merge -p \
  '{"spec":{"servingCerts":{"namedCertificates":
  [{"names":["api.$DOMAIN"],
  "servingCertificate":{"name":"api-custom-domain-cert"}}]}}}'
EOF
----

. Check the apiserver cluster CRD to check if the patch worked properly:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc get apiserver cluster -o yaml
----

. After a couple of minutes, check the certificate exposed:
+
[source,bash,subs="+macros,+attributes",role=execute]
----
echo | openssl s_client -connect api.$DOMAIN:6443 | openssl x509 -noout -text | grep Issuer
----

. Logout and login without the "--insecure-skip-tls-verify=true":
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc logout
oc login -u kubeadmin -p $AROPASS --server=$AROURL
----

== What we've accomplished so far

You now have an ARO cluster that is disconnected from the Internet for both ingress and egress.
It's accessed by tunneling through your jumphost with sshuttle for now.
It's running a custom default domain name with a custom certificate.

. Confirm that your cluster cannot egress to the Internet. This command should fail.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc debug -- curl -v https://redhat.com
----

== Opening up Internet egress

. Create firewall rules for ARO resources
+
NOTE: ARO clusters do not need access to the internet, however your own workloads running on them may.
This course requires it, so please set it up.

** For simplicity, create a Network Rule to allow all egress traffic and an Application Rule that allows all HTTP and HTTPS traffice. In practice, we would set up rules to allow only specific destinations as needed by our applications.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
az network firewall network-rule create -g $AZR_RESOURCE_GROUP -f aro-private \
  --collection-name 'allow-https' --name allow-all                          \
  --action allow --priority 100                                             \
  --source-addresses '*' --dest-addr '*'                                    \
  --protocols 'Any' --destination-ports 1-65535

az network firewall application-rule create -g $AZR_RESOURCE_GROUP -f aro-private     \
    --collection-name 'Allow_Egress'                                                  \
    --action allow                                                                    \
    --priority 100                                                                    \
    -n 'required'                                                                     \
    --source-addresses '*'                                                            \
    --protocols 'http=80' 'https=443'                                                 \
    --target-fqdns '*'
----
+
. Now confirm that your cluster can egress to the Internet.
+
[source,bash,subs="+macros,+attributes",role=execute]
----
oc debug -- curl -v https://redhat.com
----

== Congratulations

Your cluster is ready.

Let's move on to setting up AAD so you can login to ARO with your own username and password.

-> xref:100-setup/azuread-aro.adoc[Configure ARO to use Azure AD]
